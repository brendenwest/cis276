{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMd8P8WH94nEW96ABuVIoQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendenwest/cis276/blob/main/3_data_acquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Acquisition & Cleaning\n",
        "\n",
        "### Reading\n",
        "- Murach's, Chapter 5, 6\n",
        "- https://wesmckinney.com/book/accessing-data.html\n",
        "- https://wesmckinney.com/book/data-cleaning.html\n",
        "\n",
        "### Learning Outcomes\n",
        "\n",
        "- Loading data from files\n",
        "- Loading data from a database\n",
        "- loading data from the internet\n",
        "- Saving data to files\n",
        "- Handling missing values\n",
        "- Handling outliers\n",
        "- Handling ata type problems"
      ],
      "metadata": {
        "id": "bG_vbwwF_NMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Acquisition"
      ],
      "metadata": {
        "id": "NsgHeovi3OWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Data into a DataFrame\n",
        "\n",
        "Pandas can import structured data from a variety of file formats, as listed below.\n",
        "\n",
        "Some common methods and file formats are:\n",
        "\n",
        "- **read_csv** - read delimited values from .csv or .tsv files\n",
        "- **read_html** - read tables from a .html file\n",
        "- **read_excel** - read .xls, .xlsx Excel spreadsheets\n",
        "- **read_json** - read JavaScript Object Notation (JSON) from .json files\n",
        "- **read_sas** - read a dataset created by SAS\n",
        "- **read_spss** - read a data file created by SPSS\n",
        "- **read_xml** - .xml - Extensible Markup Language\n",
        "- **read_sql** - read results of an SQL query\n",
        "- **read_sql_table** - read a whole SQL table (similar to everything in a table using `read_sql`)\n",
        "\n",
        "These methods only work when the data is in a tabular form. If the data isnâ€™t tabular (e.g. with complex or nested data), the read method will throw an error.\n"
      ],
      "metadata": {
        "id": "5-FomjEZAczA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftThf-wU_MOi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# get example data\n",
        "url = \"https://data.cdc.gov/api/views/v6ab-adf5/rows.csv?accessType=DOWNLOAD\"\n",
        "mortality_data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading data to a file\n",
        "Sometimes it's helpful to retrieve a file from the internet and save to disk before reading into a DataFrame.\n",
        "\n",
        "Python's `urllib.request` module is helpful for that:"
      ],
      "metadata": {
        "id": "y0ls41sMDcDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "data_url = \"https://data.cdc.gov/api/views/v6ab-adf5/rows.csv?accessType=DOWNLOAD\"\n",
        "request.urlretrieve(data_url, filename='mortality_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sth3Z0BvDku0",
        "outputId": "e61ae8d9-749a-4846-ed9e-ea2ce5164350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mortality_data.csv', <http.client.HTTPMessage at 0x7fb86ad41820>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with JSON data"
      ],
      "metadata": {
        "id": "-jVchwdJHK2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with Databases\n",
        "\n",
        "Python has libraries for interacting with common relational database platforms:\n",
        "\n",
        "- sqlite3 - SQLite\n",
        "- pymysql - MySQL\n",
        "- psycopg2 - PostgreSQL\n",
        "- cx_oracle - Oracle\n",
        "- pymssql - MS SQL Server\n",
        "\n",
        "You can `query` a database from Python by:\n",
        "\n",
        "- creating a connection object with the `connect()` method\n",
        "- getting a cursor object with the `cursor()` method\n",
        "- executing an SQL query to fetch desired rows with `execute()` and `fetchall()`\n",
        "\n",
        "For example, to list the tables in a database:\n",
        "```\n",
        "import sqlite3\n",
        "fires_con = sqlite3.connect('Data/FPA_FOD_20170508.sqlite')\n",
        "fires_cur = fires_con.cursor()\n",
        "'SELECT name FROM sqlite_master WHERE type=\"table\"').fetchall()\n",
        "```\n",
        "\n",
        "SQL query results can be read directly into a DataFrame using the `read_sql_query` method:\n",
        "\n",
        "```\n",
        "fires = pd.read_sql_query(\n",
        "'''SELECT STATE, FIRE_YEAR, DATETIME(DISCOVERY_DATE) AS DISCOVERY_DATE, FIRE_NAME, FIRE_SIZE, LATITUDE, LONGITUDE FROM Fires''', fires_con)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "gIbFAoPTHNX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with Google Drive"
      ],
      "metadata": {
        "id": "5-aq0ADSHTZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Cleaning data is a crucial and often time-consuming step in data science.\n",
        "\n",
        "Data scientists might use pure Python, psndas, or other programming tools for this step. Examples here focus on pandas with a few other approaches for specific scenarios.\n",
        "\n",
        "Common tasks are:\n",
        "\n",
        "*   Handling missing data\n",
        "*   Simplifying data\n",
        "*   Data-type conversion"
      ],
      "metadata": {
        "id": "JKLaF7lK21_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data\n",
        "\n",
        "Often data analysts need to account for missing data values.\n",
        "\n",
        "pandas uses the floating-point value NaN (Not a Number) to represent missing numerica data. This is a **sentinel** value that can be easily detected.\n",
        "\n",
        "The built-in Python `None` value is also treated as NA.\n",
        "\n",
        "pandas has several methods for detecting `NaN` values in a Series or DataFrame:\n",
        "- isnull\n",
        "- notnull\n",
        "\n",
        "These methods can be used as filters in a data query.\n",
        "\n",
        "`data[data.notnull()]`\n",
        "\n",
        "Alternatively, programs can use `dropna` to filter axis labels where values may have missing data.\n",
        "\n",
        "`dropna` has options to control how many missing values a row or column should have to be dropped.\n",
        "\n",
        "**replace missing values**\n",
        "Sometimes it's more useful to replace missing data with a specific or interpreted value, using `fillna`.\n",
        "\n",
        "`fillna` can use a function to determine fill value.\n",
        "\n",
        "`fillna` returns a new object, but has an `inplace` option."
      ],
      "metadata": {
        "id": "u9jyggdw3up3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simplifying Data\n",
        "\n",
        "- **removing duplicates** - DataFrames have built-in methods to identify which rows are `duplicated` and to `drop_duplicates`. By default, these methods consider all columns, but programs can specify a subset.\n",
        "\n",
        "- **replacing values** - the replace() method is a simple approach for replacing values in a pandas object.\n",
        "\n",
        "- **handling outliers** - programs may want to find & replace or filter values that exceed some threshold."
      ],
      "metadata": {
        "id": "TjSf3Z5c33KI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### String Conversion\n",
        "\n",
        "python has a wide range of built-in string methods. Some common ones are:\n",
        "  - **split** - generate an array of substrings from a string based on a delimiter\n",
        "  - **lowercase** - convert a string to lower case\n",
        "  - **uppercase** - convert a string to upper case\n",
        "  - **join** - combine strings with a delimiter\n",
        "  - **index** - determine where in a string a substring is first found\n",
        "  - **find** - determine if a string contains a substring\n",
        "  - **count** - number of occurences of a substring in a string\n",
        "  - **replace** - substitute occurrences of one pattern with another.\n",
        "\n",
        "\n",
        "**Regular Expressions** provide a (mostly) language-agnostic logical syntax for finding/matching string patterns in text.\n",
        "\n",
        "`regex` patterns can be applied to strings with python's [re module](https://docs.python.org/3/library/re.html)."
      ],
      "metadata": {
        "id": "NRPgGiWB383P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"foo    bar\\t baz  \\tqux\"\n",
        "\n",
        "# inline regex pattern\n",
        "re.split('\\s+', text)\n",
        "\n",
        "# reusable regex object\n",
        "regex = re.compile('\\s+')\n",
        "regex.split(text)"
      ],
      "metadata": {
        "id": "0VmnIbSS4Cys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}